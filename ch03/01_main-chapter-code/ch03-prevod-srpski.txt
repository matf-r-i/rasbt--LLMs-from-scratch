Cell 1ae38945-39dd-45dc-ad4f-da7a4404241f:
  Додатни код за књигу "Build a Large Language Model From Scratch" аутора Sebastian Raschka
  Код репозиторијум: https://github.com/rasbt/LLMs-from-scratch

  Cell 8bfa70ec-5c4c-40e8-b923-16f8167e3181:
  Поглавље 3: Кодирање механизама пажње

  Cell c29bcbe8-a034-43a2-b557-997b03c9882d:
  Пакети који се користе у овом бележнику:

  Cell a2a4474d-7c68-4846-8702-37906cf08197:
  - Ово поглавље покрива механизме пажње, мотор LLM-ова:

  Cell ecc4dcee-34ea-4c05-9085-2f8887f70363:
  3.1 Проблем са моделирањем дугих секвенци

  Cell a55aa49c-36c2-48da-b1d9-70f416e46a6a:
  - Нема кода у овом одељку
  - Превођење текста реч по реч није изводљиво због разлика у граматичким структурама између изворног и циљног језика:

  Cell db03c48a-3429-48ea-9d4a-2e53b0e516b1:
  - Пре увођења трансформер модела, енкодер-декодер RNN-ови су се често користили за задатке машинског превођења
  - У овој поставци, енкодер обрађује секвенцу токена из изворног језика, користећи скривено стање - врсту средњег слоја унутар неуронске мреже - да генерише кондензовану репрезентацију целе улазне секвенце:

  Cell 3602c585-b87a-41c7-a324-c5e8298849df:
  3.2 Хватање зависности података помоћу механизама пажње

  Cell b6fde64c-6034-421d-81d9-8244932086ea:
  - Нема кода у овом одељку
  - Кроз механизам пажње, сегмент декодера који генерише текст је способан да селективно приступи свим улазним токенима, што имплицира да одређени улазни токени имају већи значај од других у генерисању специфичног
  излазног токена:

  Cell 8044be1f-e6a2-4a1f-a6dd-e325d3bad05e:
  - Самопажња у трансформерима је техника дизајнирана да побољша улазне репрезентације омогућавајући свакој позицији у секвенци да ангажује и одреди релевантност сваке друге позиције унутар исте секвенце

  Cell 5efe05ff-b441-408e-8d66-cde4eb3397e3:
  3.3 Обраћање пажње на различите делове улаза са самопажњом

  Cell 6d9af516-7c37-4400-ab53-34936d5495a9:
  3.3.1 Једноставан механизам самопажње без параметара за тренирање

  Cell d269e9f1-df11-4644-b575-df338cf46cdf:
  - Овај одељак објашњава веома поједностављену варијанту самопажње, која не садржи параметре за тренирање
  - Ово је чисто за илустративне сврхе и НИЈЕ механизам пажње који се користи у трансформерима
  - Следећи одељак, одељак 3.3.2, прошириће овај једноставан механизам пажње да имплементира прави механизам самопажње
  - Претпоставимо да нам је дата улазна секвенца $x^{(1)}$ до $x^{(T)}$
    - Улаз је текст (на пример, реченица као "Your journey starts with one step") који је већ претворен у уграђивања токена као што је описано у поглављу 2
    - На пример, $x^{(1)}$ је d-димензионални вектор који представља реч "Your", и тако даље
  - Циљ: израчунати контекст векторе $z^{(i)}$ за сваки елемент улазне секвенце $x^{(i)}$ у $x^{(1)}$ до $x^{(T)}$ (где $z$ и $x$ имају исту димензију)
    - Контекст вектор $z^{(i)}$ је пондерисани збир преко улаза $x^{(1)}$ до $x^{(T)}$
    - Контекст вектор је "контекст"-специфичан за одређени улаз
        - Уместо $x^{(i)}$ као чувара места за произвољан улазни токен, размотримо други улаз, $x^{(2)}$
      - И да наставимо са конкретним примером, уместо чувара места $z^{(i)}$, размотримо други излазни контекст вектор, $z^{(2)}$
      - Други контекст вектор, $z^{(2)}$, је пондерисани збир преко свих улаза $x^{(1)}$ до $x^{(T)}$ пондерисан у односу на други улазни елемент, $x^{(2)}$
      - Тежине пажње су тежине које одређују колико сваки од улазних елемената доприноси пондерисаном збиру приликом израчунавања $z^{(2)}$
      - Укратко, мислите на $z^{(2)}$ као на модификовану верзију $x^{(2)}$ која такође укључује информације о свим другим улазним елементима који су релевантни за дати задатак

  Cell fcc7c7a2-b6ab-478f-ae37-faa8eaa8049a:
  - (Молимо обратите пажњу да су бројеви на овој слици скраћени на једну цифру после децималне тачке да би се смањио визуелни хаос; слично, друге слике могу такође садржати скраћене вредности)

  Cell ff856c58-8382-44c7-827f-798040e6e697:
  - По конвенцији, ненормализоване тежине пажње се називају "резултати пажње" док се нормализовани резултати пажње, који се сабирају до 1, називају "тежине пажње"

  Cell 01b10344-128d-462a-823f-2178dff5fd58:
  - Код испод пролази кроз горњу слику корак по корак

  - Корак 1: израчунати ненормализоване резултате пажње $\omega$
  - Претпоставимо да користимо други улазни токен као упит, то јест, $q^{(2)} = x^{(2)}$, израчунавамо ненормализоване резултати пажње преко скаларних производа:
    - $\omega_{21} = x^{(1)} q^{(2)\top}$
    - $\omega_{22} = x^{(2)} q^{(2)\top}$
    - $\omega_{23} = x^{(3)} q^{(2)\top}$
    - ...
    - $\omega_{2T} = x^{(T)} q^{(2)\top}$
  - Горе, $\omega$ је грчко слово "omega" коришћено да симболизује ненормализоване резултати пажње
    - Индекс "21" у $\omega_{21}$ значи да је елемент улазне секвенце 2 коришћен као упит против елемента улазне секвенце 1

  Cell 35e55f7a-f2d0-4f24-858b-228e4fe88fb3:
  - Претпоставимо да имамо следећу улазну реченицу која је већ уграђена у 3-димензионалне векторе као што је описано у поглављу 3 (користимо веома малу димензију уграђивања овде за илустративне сврхе, тако да стане на     
  страницу без преламања линија):

  Cell 299baef3-b1a8-49ba-bad4-f62c8a416d83:
  - (У овој књизи, пратимо уобичајену конвенцију машинског учења и дубоког учења где су примери за тренирање представљени као редови, а вредности карактеристика као колоне; у случају тензора приказаног горе, сваки ред     
  представља реч, а свака колона представља димензију уграђивања)
  - Примарни циљ овог одељка је да демонстрира како се контекст вектор $z^{(2)}$ израчунава користећи другу улазну секвенцу, $x^{(2)}$, као упит
  - Слика приказује почетни корак у овом процесу, који укључује израчунавање резултата пажње ω између $x^{(2)}$ и свих других улазних елемената кроз операцију скаларног производа

  Cell 77be52fb-82fd-4886-a4c8-f24a9c87af22:
  - Користимо улазни елемент секвенце 2, $x^{(2)}$, као пример да израчунамо контекст вектор $z^{(2)}$; касније у овом одељку, генерализоваћемо ово да израчунамо све контекст векторе.
  - Први корак је да израчунамо ненормализоване резултати пажње израчунавањем скаларног производа између упита $x^{(2)}$ и свих других улазних токена:

  Cell 8df09ae0-199f-4b6f-81a0-2f70546684b8:
  - Напомена: скаларни производ је у суштини скраћеница за множење два вектора елемент по елемент и сабирање резултујућих производа:

  Cell 7d444d76-e19e-4e9a-a268-f315d966609b:
  - Корак 2: нормализовати ненормализоване резултати пажње ("омеге", $\omega$) тако да се сабирају до 1
  - Ево једноставног начина да се нормализују ненормализовани резултати пажње да се сабирају до 1 (конвенција, корисна за интерпретацију, и важна за стабилност тренирања):

  Cell 75dc0a57-f53e-41bf-8793-daa77a819431:
  - Међутим, у пракси, коришћење softmax функције за нормализацију, која је боља у руковању екстремним вредностима и има пожељнија својства градијента током тренирања, је уобичајено и препоручено.
  - Ево наивне имплементације softmax функције за скалирање, која такође нормализује елементе вектора тако да се сабирају до 1:

  Cell f0a1cbbb-4744-41cb-8910-f5c1355555fb:
  - Наивна имплементација горе може патити од проблема нумеричке нестабилности за велике или мале улазне вредности због проблема прекорачења и поткорачења
  - Стога, у пракси, препоручује се да се користи PyTorch имплементација softmax-а уместо тога, која је високо оптимизована за перформансе:

  Cell e43e36c7-90b2-427f-94f6-bb9d31b2ab3f:
  - Корак 3: израчунати контекст вектор $z^{(2)}$ множењем уграђених улазних токена, $x^{(i)}$ са тежинама пажње и сабирањем резултујућих вектора:

● Cell 5a454262-40eb-430e-9ca4-e43fb8d6cd89:
  3.3.2 Израчунавање тежина пажње за све улазне токене

  Cell 6a02bb73-fc19-4c88-b155-8314de5d63a8:
  Генерализација на све токене улазне секвенце:

  - Горе, израчунали смо тежине пажње и контекст вектор за улаз 2 (као што је илустровано у истакнутом реду на слици испод)
  - Даље, генерализујемо ово израчунавање да израчунамо све тежине пажње и контекст векторе

  Cell 11c0fb55-394f-42f4-ba07-d01ae5c98ab4:
  - (Молимо обратите пажњу да су бројеви на овој слици скраћени на две цифре после децималне тачке да би се смањио визуелни хаос; вредности у сваком реду треба да се сабирају до 1.0 или 100%; слично, цифре у другим        
  сликама су скраћене)

  Cell b789b990-fb51-4beb-9212-bf58876b5983:
  - У самопажњи, процес почиње са израчунавањем резултата пажње, који се затим нормализују да се изведу тежине пажње које укупно износе 1
  - Ове тежине пажње се затим користе за генерисање контекст вектора кроз пондерисано сабирање улаза

  Cell aa652506-f2c8-473c-a905-85c389c842cc:
  - Применити претходни корак 1 на све парне елементе да се израчуна матрица ненормализованих резултата пажње:

  Cell 1539187f-1ece-47b7-bc9b-65a97115f1d4:
  - Можемо постићи исто као горе ефикасније преко множења матрица:

  Cell 02c4bac4-acfd-427f-9b11-c436ac71748d:
  - Слично кораку 2 претходно, нормализујемо сваки ред тако да вредности у сваком реду имају збир 1:

  Cell 3fa6d02b-7f15-4eb4-83a7-0b8a819e7a0c:
  - Брза провера да вредности у сваком реду заиста имају збир 1:

  Cell 138b0b5c-d813-44c7-b373-fde9540ddfd1:
  - Применити претходни корак 3 да се израчунају сви контекст вектори:

  Cell 25b245b8-7732-4fab-aa1c-e3d333195605:
  - Као проверу, претходно израчунати контекст вектор $z^{(2)} = [0.4419, 0.6515, 0.5683]$ може се наћи у 2. реду горе:

  Cell a303b6fb-9f7e-42bb-9fdb-2adabf0a6525:
  3.4 Имплементација самопажње са параметрима за тренирање

  Cell 88363117-93d8-41fb-8240-f7cfe08b14a3:
  - Концептуални оквир који илуструје како се механизам самопажње развијен у овом одељку интегрише у целокупну нарацију и структуру ове књиге и поглавља

  Cell 2b90a77e-d746-4704-9354-1ddad86e6298:
  3.4.1 Израчунавање тежина пажње корак по корак

  Cell 46e95a46-1f67-4b71-9e84-8e2db84ab036:
  - У овом одељку, имплементирамо механизам самопажње који се користи у оригиналној архитектури трансформера, GPT моделима, и већини других популарних LLM-ова
  - Овај механизам самопажње се такође назива "скалирана пажња скаларног производа"
  - Општа идеја је слична као пре:
    - Желимо да израчунамо контекст векторе као пондерисане збирове преко улазних вектора специфичних за одређени улазни елемент
    - За горње, потребне су нам тежине пажње
  - Као што ћете видети, постоје само мале разлике у поређењу са основним механизмом пажње уведеним раније:
    - Најзначајнија разлика је увођење матрица тежина које се ажурирају током тренирања модела
    - Ове матрице тежина за тренирање су кључне тако да модел (посебно, модул пажње унутар модела) може научити да производи "добре" контекст векторе

  Cell 4d996671-87aa-45c9-b2e0-07a7bcc9060a:
  - Имплементација механизма самопажње корак по корак, почећемо увођењем три матрице тежина за тренирање $W_q$, $W_k$, и $W_v$
  - Ове три матрице се користе за пројекцију уграђених улазних токена, $x^{(i)}$, у векторе упита, кључа и вредности преко множења матрица:

    - Вектор упита: $q^{(i)} = W_q x^{(i)}$
    - Вектор кључа: $k^{(i)} = W_k x^{(i)}$
    - Вектор вредности: $v^{(i)} = W_v x^{(i)}$

  Cell 9f334313-5fd0-477b-8728-04080a427049:
  - Димензије уграђивања улаза $x$ и вектора упита $q$ могу бити исте или различите, зависно од дизајна модела и специфичне имплементације
  - У GPT моделима, улазне и излазне димензије су обично исте, али за илустративне сврхе, да боље пратимо израчунавање, бирамо различите улазне и излазне димензије овде:

  Cell f528cfb3-e226-47dd-b363-cc2caaeba4bf:
  - Испод, иницијализујемо три матрице тежина; обратите пажњу да постављамо requires_grad=False да смањимо хаос у излазима за илустративне сврхе, али ако бисмо користили матрице тежина за тренирање модела, поставили       
  бисмо requires_grad=True да ажурирамо ове матрице током тренирања модела

  Cell abfd0b50-7701-4adb-821c-e5433622d9c4:
  - Даље израчунавамо векторе упита, кључа и вредности:

  Cell 9be308b3-aca3-421b-b182-19c3a03b71c7:
  - Као што можемо видети испод, успешно смо пројектовали 6 улазних токена из 3D на 2D простор уграђивања:

  Cell bac5dfd6-ade8-4e7b-b0c1-bed40aa24481:
  - У следећем кораку, корак 2, израчунавамо ненормализоване резултате пажње израчунавањем скаларног производа између упита и сваког вектора кључа:

  Cell 9e9d15c0-c24e-4e6f-a160-6349b418f935:
  - Пошто имамо 6 улаза, имамо 6 резултата пажње за дати вектор упита:

  Cell e1609edb-f089-461a-8de2-c20c1bb29836:
  - Даље, у кораку 3, израчунавамо тежине пажње (нормализоване резултати пажње који се сабирају до 1) користећи softmax функцију коју смо користили раније
  - Разлика у односу на раније је што сада скалирамо резултате пажње дељењем их са квадратним кореном димензије уграђивања, $\sqrt{d_k}$ (тј. d_k**0.5):

  Cell 1890e3f9-db86-4ab8-9f3b-53113504a61f:
  - У кораку 4, сада израчунавамо контекст вектор за улазни вектор упита 2:

  Cell 9d7b2907-e448-473e-b46c-77735a7281d8:
  3.4.2 Имплементација компактне класе SelfAttention

  Cell 04313410-3155-4d90-a7a3-2f3386e73677:
  - Све заједно, можемо имплементирати механизам самопажње на следећи начин:

  Cell 048e0c16-d911-4ec8-b0bc-45ceec75c081:
  - Можемо поједноставити имплементацију горе користећи PyTorch-ове Linear слојеве, који су еквивалентни множењу матрица ако онемогућимо јединице пристрасности
  - Још једна велика предност коришћења nn.Linear уместо нашег ручног приступа nn.Parameter(torch.rand(...) је што nn.Linear има преферирану шему иницијализације тежина, што доводи до стабилнијег тренирања модела

  Cell 915cd8a5-a895-42c9-8b8e-06b5ae19ffce:
  - Напомена да SelfAttention_v1 и SelfAttention_v2 дају различите излазе јер користе различите почетне тежине за матрице тежина

  Cell c5025b37-0f2c-4a67-a7cb-1286af7026ab:
  3.5 Сакривање будућих речи са каузалном пажњом

  Cell aef0a6b8-205a-45bf-9d26-8fd77a8a03c3:
  - У каузалној пажњи, тежине пажње изнад дијагонале су маскиране, осигуравајући да за било који дати улаз, LLM није у могућности да користи будуће токене док израчунава контекст векторе са тежином пажње

  Cell 82f405de-cd86-4e72-8f3c-9ea0354946ba:
  3.5.1 Примена маске каузалне пажње

  Cell 014f28d0-8218-48e4-8b9c-bdc5ce489218:
  - У овом одељку, претварамо претходни механизам самопажње у механизам каузалне самопажње
  - Каузална самопажња осигурава да предвиђање модела за одређену позицију у секвенци зависи само од познатих излаза на претходним позицијама, а не од будућих позиција
  - Једноставније речено, ово осигурава да свако следеће предвиђање речи треба да зависи само од претходних речи
  - Да бисмо ово постигли, за сваки дати токен, маскирамо будуће токене (оне који долазе после тренутног токена у улазном тексту):

  Cell cbfaec7a-68f2-4157-a4b5-2aeceed199d9:
  - Да илуструјемо и имплементирамо каузалну самопажњу, хајде да радимо са резултатима пажње и тежинама из претходног одељка:

  Cell 89020a96-b34d-41f8-9349-98c3e23fd5d6:
  - Најједноставнији начин да се маскирају будуће тежине пажње је креирањем маске преко PyTorch-ове tril функције са елементима испод главне дијагонале (укључујући и саму дијагоналу) постављеним на 1 и изнад главне        
  дијагонале постављеним на 0:

  Cell efce2b08-3583-44da-b3fc-cabdd38761f6:
  - Затим, можемо помножити тежине пажње са овом маском да поставимо на нулу резултате пажње изнад дијагонале:

  Cell 3eb35787-cf12-4024-b66d-e7215e175500:
  - Међутим, ако би се маска применила после softmax-а, као горе, то би пореметило расподелу вероватноће креирану softmax-ом
  - Softmax осигурава да сви излазни вредности имају збир 1
  - Маскирање после softmax-а би захтевало поновну нормализацију излаза да се сабирају до 1 поново, што компликује процес и може довести до нежељених ефеката

  Cell 94db92d7-c397-4e42-bd8a-6a2b3e237e0f:
  - Да бисмо осигурали да редови имају збир 1, можемо нормализовати тежине пажње на следећи начин:

  Cell 512e7cf4-dc0e-4cec-948e-c7a3c4eb6877:
  - Док смо технички завршили са кодирањем механизма каузалне пажње сада, погледајмо укратко ефикаснији приступ да постигнемо исто као горе
  - Дакле, уместо постављања на нулу тежина пажње изнад дијагонале и поновне нормализације резултата, можемо маскирати ненормализоване резултате пажње изнад дијагонале са негативном бесконачношћу пре него што уђу у        
  softmax функцију:

  Cell 91d5f803-d735-4543-b9da-00ac10fb9c50:
  - Као што можемо видети испод, сада тежине пажње у сваком реду тачно имају збир 1 поново:

● Cell 7636fc5f-6bc6-461e-ac6a-99ec8e3c0912:
  3.5.2 Маскирање додатних тежина пажње са dropout-ом

  Cell ec3dc7ee-6539-4fab-804a-8f31a890c85a:
  - Поред тога, такође примењујемо dropout да смањимо претерано уклапање током тренирања
  - Dropout се може применити на неколико места:
    - на пример, после израчунавања тежина пажње;
    - или после множења тежина пажње са векторима вредности
  - Овде, примењиваћемо dropout маску после израчунавања тежина пажње јер је то уобичајеније
  - Штавише, у овом конкретном примеру, користимо dropout стопу од 50%, што значи насумично маскирање половине тежина пажње. (Када тренирамо GPT модел касније, користићемо нижу dropout стопу, као што је 0.1 или 0.2        

  Cell 5a575458-a6da-4e54-8688-83e155f2de06:
  - Ако применимо dropout стопу од 0.5 (50%), вредности које нису испуштене ће бити скалиране одговарајуће фактором 1/0.5 = 2.

  Cell 269df5c8-3e25-49d0-95d3-bb232287404f:
  - Напомена да резултујући dropout излази могу изгледати различито зависно од вашег оперативног система; можете прочитати више о овој недоследности https://github.com/pytorch/pytorch/issues/121595

  Cell cdc14639-5f0f-4840-aa9d-8eb36ea90fb7:
  3.5.3 Имплементација компактне класе каузалне самопажње

  Cell 09c41d29-1933-43dc-ada6-2dbb56287204:
  - Сада, спремни смо да имплементирамо радну имплементацију самопажње, укључујући каузалну и dropout маске
  - Још једна ствар је да имплементирамо код да рукује серијама које се састоје од више од једног улаза тако да наша CausalAttention класа подржава серијске излазе произведене од стране учитавача података који смо
  имплементирали у поглављу 2
  - Ради једноставности, да симулирамо такав серијски улаз, дуплирамо пример улазног текста:

  Cell c4333d12-17e4-4bb5-9d83-54b3a32618cd:
  - Напомена да се dropout примењује само током тренирања, не током закључивања

  Cell c8bef90f-cfd4-4289-b0e8-6a00dc9be44c:
  3.6 Проширење једно-главе пажње на више-главу пажњу

  Cell 11697757-9198-4a1c-9cee-f450d8bbd3b9:
  3.6.1 Налагање више слојева једно-главе пажње

  Cell 70766faf-cd53-41d9-8a17-f1b229756a5a:
  - Испод је резиме самопажње имплементиране претходно (каузалне и dropout маске нису приказане ради једноставности)
  - Ово се такође назива једно-глава пажња:
  - Једноставно налажемо више модула једно-главе пажње да добијемо модул више-главе пажње:
  - Главна идеја иза више-главе пажње је да се покрене механизам пажње више пута (паралелно) са различитим, наученим линеарним пројекцијама. Ово омогућава моделу да заједнички обраћа пажњу на информације из различитих     
  подпростора репрезентације на различитим позицијама.

  Cell 193d3d2b-2578-40ba-b791-ea2d49328e48:
  - У имплементацији горе, димензија уграђивања је 4, јер имамо d_out=2 као димензију уграђивања за векторе кључа, упита и вредности као и за контекст вектор. И пошто имамо 2 главе пажње, имамо излазну димензију
  уграђивања 2*2=4

  Cell 6836b5da-ef82-4b4c-bda1-72a462e48d4e:
  3.6.2 Имплементација више-главе пажње са поделама тежина

  Cell f4b48d0d-71ba-4fa0-b714-ca80cabcb6f7:
  - Док је горе интуитивна и потпуно функционална имплементација више-главе пажње (омотавајући имплементацију једно-главе пажње CausalAttention из раније), можемо написати самосталну класу под називом
  MultiHeadAttention да постигнемо исто
  - Не спајамо појединачне главе пажње за ову самосталну класу MultiHeadAttention
  - Уместо тога, креирамо појединачне W_query, W_key, и W_value матрице тежина и затим поделимо оне у индивидуалне матрице за сваку главу пажње:

  Cell d334dfb5-2b6c-4c33-82d5-b4e9db5867bb:
  - Напомена да је горе у суштини преписана верзија MultiHeadAttentionWrapper која је ефикаснија
  - Резултујући излаз изгледа помало различито пошто се случајне иницијализације тежина разликују, али обе су потпуно функционалне имплементације које се могу користити у GPT класи коју ћемо имплементирати у наредним      
  поглављима
  - Напомена да смо поред тога додали линеарни пројекциони слој (self.out_proj ) класи MultiHeadAttention горе. Ово је једноставно линеарна трансформација која не мења димензије. То је стандардна конвенција да се
  користи такав пројекциони слој у LLM имплементацији, али није строго неопходан (скорашње истраживање је показало да се може уклонити без утицаја на перформансе моделовања; погледајте одељак за даље читање на крају       
  овог поглавља)

  Cell 8b0ed78c-e8ac-4f8f-a479-a98242ae8f65:
  - Напомена да ако сте заинтересовани за компактну и ефикасну имплементацију горе, можете такође размотрити класу https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html у PyTorch-у

  Cell 363701ad-2022-46c8-9972-390d2a2b9911:
  - Пошто горња имплементација може изгледати помало сложена на први поглед, погледајмо шта се дешава када се изврши attn_scores = queries @ keys.transpose(2, 3):

  Cell 0587b946-c8f2-4888-adbf-5a5032fbfd7b:
  - У овом случају, имплементација множења матрица у PyTorch-у ће руковати 4-димензионалним улазним тензором тако да се множење матрица изврши између 2 последње димензије (num_tokens, head_dim) и затим понови за
  индивидуалне главе
  - На пример, следеће постаје компактнији начин да се израчуна множење матрица за сваку главу одвојено:

  Cell dec671bf-7938-4304-ad1e-75d9920e7f43:
  Резиме и закључци

  Cell fa3e4113-ffca-432c-b3ec-7a50bd15da25:
  - Погледајте ./multihead-attention.ipynb код бележник, који је концизна верзија учитавача података (поглавље 2) плус класа више-главе пажње коју смо имплементирали у овом поглављу и која ће бити потребна за тренирање    
   GPT модела у наредним поглављима
  - Решења вежби можете наћи у ./exercise-solutions.ipynb
