12. Ћелија 14:
  - У овом одељку токенизирамо текст, што значи разбијање текста на мање јединице, као што су појединачне речи и знакови интерпункције

  14. Ћелија 16:
  - Учитајте сирови текст са којим желимо да радимо
  - ["The Verdict" од Edith Wharton](https://en.wikisource.org/wiki/The_Verdict) је кратка прича у јавном домену

  16. Ћелија 18:
  - Циљ је токенизирати и претворити овај текст у векторске репрезентације за LLM
  - Развијмо једноставан токенизатор на основу неког једноставног узорка текста који ћемо касније применити на горе наведени текст
  - Следећи регуларни израз ће поделити текст на белине

  18. Ћелија 20:
  - Не желимо само да поделимо на белине већ и на зарезе и тачке, па хајде да модификујемо регуларни израз да то такође ради

  20. Ћелија 22:
  - Као што можемо видети, ово ствара празне стрингове, хајде да их уклонимо

  22. Ћелија 24:
  - Ово изгледа прилично добро, али хајде да такође обрадимо друге врсте интерпункције, као што су тачке, знакови питања, итд.

  24. Ћелија 26:
  - Ово је прилично добро, и сада смо спремни да применимо ову токенизацију на сирови текст

  26. Ћелија 28:
  - Хајде да израчунамо укупан број токена

  28. Ћелија 30:
  ## 2.3 Претварање токена у ID-ове токена

  29. Ћелија 31:
  - Затим претварамо текстуалне токене у ID-ове токена које можемо обрадити преко слојева за векторске репрезентације касније

  31. Ћелија 33:
  - Од ових токена сада можемо изградити речник који се састоји од свих јединствених токена

  33. Ћелија 35:
  - Испод су први 50 уноса у овом речнику:

  35. Ћелија 37:
  - Испод илуструјемо токенизацију кратког узорка текста користећи мали речник:

  37. Ћелија 39:
  - Сада све то састављамо у класу токенизатора

  39. Ћелија 41:
  - Функција `encode` претвара текст у ID-ове токена
  - Функција `decode` претвара ID-ове токена назад у текст

  41. Ћелија 43:
  - Можемо користити токенизатор да кодира (тј. токенизира) текстове у целе бројеве
  - Ови цели бројеви се затим могу претворити у векторске репрезентације (касније) као улаз за LLM

  43. Ћелија 45:
  - Можемо декодирати целе бројеве назад у текст

  45. Ћелија 47:
  ## 2.4 Додавање специјалних контекстних токена

  46. Ћелија 48:
  - Корисно је додати неке "специјалне" токене за непознате речи и за означавање краја текста

  48. Ћелија 50:
  - Неки токенизатори користе специјалне токене да помогну LLM-овима са додатним контекстом
  - Неки од ових специјалних токена су:
    - `[BOS]` (почетак секвенце) означава почетак текста
    - `[EOS]` (крај секвенце) означава где се текст завршава (ово се обично користи за спајање више неповезаних текстова, нпр. два различита чланка на Википедији или две различите књиге, итд.)
    - `[PAD]` (попуњавање) ако тренирамо LLM-ове са величином групе већом од 1 (можемо укључити више текстова различитих дужина; са токеном за попуњавање попуњавамо краће текстове до дужине најдужег текста тако да сви
  текстови имају једнаку дужину)
  - `[UNK]` за представљање речи које нису укључене у речник

  - Имајте на уму да GPT-2 не треба ниједан од горе поменутих токена већ користи само токен `<|endoftext|>` да смањи сложеност
  - Токен `<|endoftext|>` је аналоган горе поменутом токену `[EOS]`
  - GPT такође користи `<|endoftext|>` за попуњавање (пошто обично користимо маску када тренирамо на груписаним улазима, не бисмо обраћали пажњу на попуњене токене у сваком случају, тако да није битно који су то токени)
  - GPT-2 не користи токен `<UNK>` за речи ван речника; уместо тога, GPT-2 користи токенизатор кодирања парова бајтова (BPE), који разлаже речи на подјединице речи о којима ћемо говорити у каснијем одељку

  50. Ћелија 52:
  - Користимо токене `<|endoftext|>` између два независна извора текста:

  52. Ћелија 54:
  - Хајде да видимо шта се дешава ако токенизирамо следећи текст:

  54. Ћелија 56:
  - Горе наведено производи грешку јер реч "Hello" није садржана у речнику
  - Да бисмо се изборили са таквим случајевима, можемо додати специјалне токене као што је `"<|unk|>"` у речник да представимо непознате речи
  - Пошто већ проширујемо речник, хајде да додамо још један токен под називом `"<|endoftext|>"` који се користи у тренирању GPT-2 за означавање краја текста (и такође се користи између спојеног текста, као што је када се
  наши скупови података за тренирање састоје од више чланака, књига, итд.)

  56. Ћелија 58:
  - Такође морамо прилагодити токенизатор у складу са тим тако да зна када и како да користи нови `<unk>` токен

  58. Ћелија 60:
  Хајде да покушамо да токенизирамо текст са модификованим токенизатором:

  60. Ћелија 62:
  ## 2.5 Кодирање парова бајтова (BytePair encoding)

  61. Ћелија 63:
  - GPT-2 је користио кодирање парова бајтова (BPE) као свој токенизатор
  - омогућава моделу да разложи речи које нису у његовом унапред дефинисаном речнику на мање подјединице речи или чак појединачне карактере, омогућавајући му да се носи са речима ван речника
  - На пример, ако речник GPT-2 нема реч "unfamiliarword", можда ће је токенизирати као ["unfam", "iliar", "word"] или неки други разлагање на подјединице речи, у зависности од његових тренираних BPE спајања
  - Оригинални BPE токенизатор можете наћи овде: [https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)
  - У овом поглављу користимо BPE токенизатор из OpenAI-јевог open-source [tiktoken](https://github.com/openai/tiktoken) библиотеке, који имплементира своје основне алгоритме у Rust-у да побољша рачунарску перформансу
  - Направио сам свесак у [./bytepair_encoder](../02_bonus_bytepair-encoder) који упоређује ове две имплементације једна поред друге (tiktoken је био око 5x бржи на узорку текста)

  63. Ћелија 65:
  - BPE токенизатори разлажу непознате речи на подречи и појединачне карактере:

  65. Ћелија 67:
  ## 2.6 Узорковање података са клизним прозором

  66. Ћелија 68:
  - Тренирамо LLM-ове да генеришу једну реч по једну реч, па желимо да припремимо податке за тренирање у складу са тим где следећа реч у секвенци представља циљ за предвиђање:

  68. Ћелија 70:
  - За сваки део текста желимо улазе и циљеве
  - Пошто желимо да модел предвиди следећу реч, циљеви су улази померени за једну позицију удесно

  70. Ћелија 72:
  - Један по један, предвиђање би изгледало овако:

  72. Ћелија 74:
  - Бринућемо се о предвиђању следеће речи у каснијем поглављу након што покријемо механизам пажње
  - За сада имплементирамо једноставан учитавач података који итерира преко улазног скупа података и враћа улазе и циљеве померене за један

  74. Ћелија 76:
  - Инсталирајте и импортујте PyTorch (погледајте Додатак А за савете о инсталацији)

  76. Ћелија 78:
  - Користимо приступ клизног прозора, мењајући позицију за +1:

  78. Ћелија 80:
  - Креирајте скуп података и учитавач података који извлаче делове из улазног скупа података текста

  80. Ћелија 82:
  - Хајде да тестирамо учитавач података са величином групе 1 за LLM са величином контекста 4:

  82. Ћелија 84:
  - Пример коришћења корака једнаког дужини контекста (овде: 4) као што је приказано испод:

  84. Ћелија 86:
  - Такође можемо креирати груписане излазе
  - Имајте на уму да овде повећавамо корак тако да немамо преклапања између група, пошто више преклапања може довести до повећаног претераног учења

  86. Ћелија 88:
  ## 2.7 Креирање векторских репрезентација токена

  87. Ћелија 89:
  - Подаци су већ скоро спремни за LLM
  - Али на крају хајде да претворимо токене у континуалну векторску репрезентацију користећи слој за векторске репрезентације
  - Обично су ови слојеви за векторске репрезентације део самог LLM-а и ажурирају се (тренирају) током тренирања модела

  89. Ћелија 91:
  - Претпоставимо да имамо следећа четири улазна примера са улазним ID-овима 2, 3, 5 и 1 (након токенизације):

  91. Ћелија 93:
  - Ради једноставности, претпоставимо да имамо мали речник од само 6 речи и желимо да креирамо векторске репрезентације величине 3:

  93. Ћелија 95:
  - Ово би резултирало матрицом тежина 6x3:

  95. Ћелија 97:
  - За оне који су упознати са one-hot кодирањем, приступ са слојем за векторске репрезентације је у суштини само ефикаснији начин имплементације one-hot кодирања праћеног множењем матрица у потпуно повезаном слоју, што
  је описано у додатном коду у [./embedding_vs_matmul](../03_bonus_embedding-vs-matmul)
  - Пошто је слој за векторске репрезентације само ефикаснија имплементација која је еквивалентна приступу one-hot кодирања и множења матрица, може се посматрати као слој неуронске мреже који се може оптимизовати преко
  пропагације уназад

  97. Ћелија 99:
  - Да бисмо претворили токен са ID-ом 3 у 3-димензионални вектор, урадимо следеће:

  99. Ћелија 101:
  - Имајте на уму да је горе наведено 4. ред у матрици тежина `embedding_layer`
  - Да бисмо претворили у векторске репрезентације све четири горе наведене вредности `input_ids`, урадимо

  101. Ћелија 103:
  - Слој за векторске репрезентације је у суштини операција претраживања:

  103. Ћелија 105:
  - **Можда ће вас занимати додатни садржај који упоређује слојеве за векторске репрезентације са регуларним линеарним слојевима: [../03_bonus_embedding-vs-matmul](../03_bonus_embedding-vs-matmul)**      

  105. Ћелија 107:
  ## 2.8 Кодирање позиција речи

  106. Ћелија 108:
  - Слој за векторске репрезентације претвара ID-ове у идентичне векторске репрезентације без обзира на то где се налазе у улазној секвенци:

  108. Ћелија 110:
  - Позиционе векторске репрезентације се комбинују са векторском репрезентацијом токена да формирају улазне векторске репрезентације за велики језички модел:

  110. Ћелија 112:
  - Токенизатор кодирања парова бајтова има величину речника од 50,257:
  - Претпоставимо да желимо да кодирамо улазне токене у 256-димензионалну векторску репрезентацију:

  112. Ћелија 114:
  - Ако узоркујемо податке из учитавача података, претварамо токене у свакој групи у 256-димензионални вектор
  - Ако имамо величину групе 8 са по 4 токена, ово резултира тензором 8 x 4 x 256:

  114. Ћелија 116:
  - GPT-2 користи апсолутне позиционе векторске репрезентације, па једноставно креирамо још један слој за векторске репрезентације:

  116. Ћелија 118:
  - Да бисмо креирали улазне векторске репрезентације које се користе у LLM-у, једноставно додамо векторске репрезентације токена и позиционе векторске репрезентације:

  118. Ћелија 120:
  - У почетној фази процеса обраде улаза, улазни текст се сегментира на одвојене токене
  - Након ове сегментације, ови токени се трансформишу у ID-ове токена на основу унапред дефинисаног речника:

  120. Ћелија 122:
  # Резиме и закључци

  121. Ћелија 123:
  Погледајте свесак са кодом [./dataloader.ipynb](./dataloader.ipynb), који је сажета верзија учитавача података који смо имплементирали у овом поглављу и који ће нам требати за тренирање GPT модела у наредним поглављима.

  Погледајте [./exercise-solutions.ipynb](./exercise-solutions.ipynb) за решења вежби.

