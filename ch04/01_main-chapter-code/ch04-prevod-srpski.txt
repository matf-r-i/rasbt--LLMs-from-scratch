
  Cell 08f4321d-d32a-4a90-bfc7-e923f316b2f8:
  Додатни код за књигу "Build a Large Language Model From Scratch" аутора Sebastian Raschka
  Код репозиторијум: https://github.com/rasbt/LLMs-from-scratch

  Cell ce9295b2-182b-490b-8325-83a67c4a001d:
  Поглавље 4: Имплементација GPT модела од нуле за генерисање текста

  Cell e7da97ed-e02f-4d7f-b68e-a0eba3716e02:
  - У овом поглављу, имплементирамо LLM архитектуру сличну GPT-у; следеће поглавље ће се фокусирати на тренирање овог LLM-а

  Cell 53fe99ab-0bcf-4778-a6b5-6db81fb826ef:
  4.1 Кодирање LLM архитектуре

  Cell ad72d1ff-d82d-4e33-a88e-3c1a8831797b:
  - Поглавље 1 је разговарало о моделима као што су GPT и Llama, који генеришу речи секвенцијално и засновани су на декодер делу оригиналне трансформер архитектуре
  - Стога, ови LLM-ови се често називају "декодер-сличним" LLM-овима
  - У поређењу са конвенционалним моделима дубоког учења, LLM-ови су већи, углавном због њиховог огромног броја параметара, а не количине кода
  - Видећемо да су многи елементи поновљени у LLM архитектури

  Cell 0d43f5e2-fb51-434a-b9be-abeef6b98d99:
  - У претходним поглављима, користили смо мале димензије уграђивања за улазне и излазне токене ради лакоће илустрације, осигуравајући да стану на једну страницу
  - У овом поглављу, разматрамо димензије уграђивања и величине модела сличне малом GPT-2 моделу
  - Специфично ћемо кодирати архитектуру најмањег GPT-2 модела (124 милиона параметара), као што је наведено у Radford et al.-овом раду
  https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf (имајте на уму да почетни извештај наводи 117M параметара, али је ово касније исправљено у репозиторијуму тежина      
  модела)
  - Поглавље 6 ће показати како учитати предтреиране тежине у нашу имплементацију, која ће бити компатибилна са величинама модела од 345, 762 и 1542 милиона параметара

  Cell 21baa14d-24b8-4820-8191-a2808f7fbabc:
  - Детаљи конфигурације за GPT-2 модел са 124 милиона параметара укључују:

  Cell c12fcd28-d210-4c57-8be6-06cfcd5d73a4:
  - Користимо кратке имена променљивих да избегнемо дуге линије кода касније
  - "vocab_size" означава величину вокабулара од 50,257 речи, подржано од стране BPE токенизатора разматраног у Поглављу 2
  - "context_length" представља максимални број улазних токена модела, као што је омогућено позиционим уграђивањима покривеним у Поглављу 2
  - "emb_dim" је величина уграђивања за улазне токене, претварајући сваки улазни токен у 768-димензионални вектор
  - "n_heads" је број глава пажње у механизму више-главе пажње имплементираном у Поглављу 3
  - "n_layers" је број трансформер блокова унутар модела, које ћемо имплементирати у наредним одељцима
  - "drop_rate" је интензитет dropout механизма, разматраног у Поглављу 3; 0.1 значи испуштање 10% скривених јединица током тренирања да се ублажи претерано уклапање
  - "qkv_bias" одлучује да ли Linear слојеви у механизму више-главе пажње (из Поглавља 3) треба да укључују вектор пристрасности приликом израчунавања query (Q), key (K), и value (V) тензора; онемогућићемо ову опцију,     
  што је стандардна пракса у модерним LLM-овима; међутим, вратићемо се на ово касније када учитавамо предтреиране GPT-2 тежине из OpenAI-ја у нашу реимплементацију у поглављу 5

  Cell f8fad0fe-895d-4493-9e48-962e2d46c66f:

  Напомена

  - Ако покрећете овај код на Windows или Linux-у, резултујуће вредности горе могу изгледати овако:

  Output shape: torch.Size([2, 4, 50257])
  tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],
           [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],
           [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],
           [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],

          [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],
           [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],
           [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],
           [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],
         grad_fn=<UnsafeViewBackward0>)

  - Пошто су ово само случајни бројеви, ово није разлог за забринутост, и можете наставити са остатком поглавља без проблема

  ---
  Cell f8332a00-98da-4eb4-b882-922776a89917:
  4.2 Нормализација активација са layer normalization

  Cell 066cfb81-d59b-4d95-afe3-e43cf095f292:
  - Layer normalization, такође познат као LayerNorm (https://arxiv.org/abs/1607.06450), центрира активације слоја неуронске мреже око средње вредности 0 и нормализује њихову варијансу на 1
  - Ово стабилизује тренирање и омогућава бржу конвергенцију ка ефективним тежинама
  - Layer normalization се примењује и пре и после више-главе пажње модула унутар трансформер блока, који ћемо имплементирати касније; такође се примењује пре финалног излазног слоја

  Cell 5ab49940-6b35-4397-a80e-df8d092770a7:
  - Хајде да видимо како layer normalization функционише проласком малог улазног узорка кроз једноставан слој неуронске мреже:

  Cell 8fccc29e-71fc-4c16-898c-6137c6ea5d2e:
  - Хајде да израчунамо средњу вредност и варијансу за сваки од 2 улаза горе:

  Cell 052eda3e-b395-48c4-acd4-eb8083bab958:
  - Нормализација се примењује на сваки од два улаза (редова) независно; коришћење dim=-1 примењује израчунавање преко последње димензије (у овом случају, димензије карактеристика) уместо димензије реда

  Cell 9f8ecbc7-eb14-4fa1-b5d0-7e1ff9694f99:
  - Одузимање средње вредности и дељење квадратним кореном варијансе (стандардна девијација) центрира улазе да имају средњу вредност 0 и варијансу 1 преко колоне (карактеристике) димензије:

  Cell ac62b90c-7156-4979-9a79-ce1fb92969c1:
  - Сваки улаз је центриран на 0 и има јединичну варијансу 1; да побољшамо читљивост, можемо онемогућити PyTorch-ову научну нотацију:

  Cell 944fb958-d4ed-43cc-858d-00052bb6b31a:
  - Горе, нормализовали смо карактеристике сваког улаза
  - Сада, користећи исту идеју, можемо имплементирати LayerNorm класу:

  Cell e56c3908-7544-4808-b8cb-5d0a55bcca72:
  Скалирање и померање

  - Имајте на уму да поред извођења нормализације одузимањем средње вредности и дељењем варијансом, додали смо два параметра за тренирање, scale и shift параметар
  - Почетне scale (множење са 1) и shift (додавање 0) вредности немају никакав ефекат; међутим, scale и shift су параметри за тренирање које LLM аутоматски прилагођава током тренирања ако се утврди да би то побољшало      
  перформансе модела на његовом задатку тренирања
  - Ово омогућава моделу да научи одговарајуће скалирање и померање које најбоље одговарају подацима које обрађује
  - Имајте на уму да такође додајемо мању вредност (eps) пре израчунавања квадратног корена варијансе; ово је да избегнемо грешке дељења-са-нулом ако је варијанса 0

  Пристрасна варијанса
  - У израчунавању варијансе горе, постављање unbiased=False значи коришћење формуле $\frac{\sum_i (x_i - \bar{x})^2}{n}$ за израчунавање варијансе где је n величина узорка (овде, број карактеристика или колона); ова      
  формула не укључује Bessel-ову корекцију (која користи n-1 у имениоцу), дајући пристрасну процену варијансе
  - За LLM-ове, где је димензија уграђивања n веома велика, разлика између коришћења n и n-1 је занемарљива
  - Међутим, GPT-2 је трениран са пристрасном варијансом у нормализационим слојевима, што је разлог зашто смо такође усвојили ову поставку због компатибилности са предтреираним тежинама које ћемо учитати у каснијим        
  поглављима
  - Хајде сада да испробамо LayerNorm у пракси:

  Cell 11190e7d-8c29-4115-824a-e03702f9dd54:
  4.3 Имплементација feed forward мреже са GELU активацијама

  Cell b0585dfb-f21e-40e5-973f-2f63ad5cb169:
  - У овом одељку, имплементирамо мали подмодул неуронске мреже који се користи као део трансформер блока у LLM-овима
  - Почињемо са активационом функцијом
  - У дубоком учењу, ReLU (Rectified Linear Unit) активационе функције се често користе због њихове једноставности и ефикасности у различитим архитектурама неуронских мрежа
  - У LLM-овима, користе се различите друге врсте активационих функција поред традиционалног ReLU-а; два значајна примера су GELU (Gaussian Error Linear Unit) и SwiGLU (Swish-Gated Linear Unit)
  - GELU и SwiGLU су сложеније, глатке активационе функције које укључују Gaussian и sigmoid-gated линеарне јединице, респективно, нудећи боље перформансе за моделе дубоког учења, за разлику од једноставније, део по       
  део линеарне функције ReLU-а

  Cell 7d482ce7-e493-4bfc-a820-3ea99f564ebc:
  - GELU (https://arxiv.org/abs/1606.08415) се може имплементирати на неколико начина; тачна верзија је дефинисана као GELU(x)=x⋅Φ(x), где је Φ(x) кумулативна дистрибутивна функција стандардне Gaussian дистрибуције.       
  - У пракси, уобичајено је имплементирати рачунски јефтинију апроксимацију: $\text{GELU}(x) \approx 0.5 \cdot x \cdot \left(1 + \tanh\left[\sqrt{\frac{2}{\pi}} \cdot \left(x + 0.044715 \cdot x^3\right)\right]\right)      
  $ (оригинални GPT-2 модел је такође трениран са овом апроксимацијом)

  Cell 1cd01662-14cb-43fd-bffd-2d702813de2d:
  - Као што можемо видети, ReLU је део по део линеарна функција која директно излази улаз ако је позитиван; у супротном, излази нула
  - GELU је глатка, не-линеарна функција која апроксимира ReLU али са не-нула градијентом за негативне вредности (осим на приближно -0.75)
  - Даље, хајде да имплементирамо мали модул неуронске мреже, FeedForward, који ћемо користити у LLM-овом трансформер блоку касније:

  Cell 4ffcb905-53c7-4886-87d2-4464c5fecf89:
  4.4 Додавање shortcut конекција

  Cell ffae416c-821e-4bfa-a741-8af4ba5db00e:
  - Даље, хајде да разговарамо о концепту иза shortcut конекција, такође названих skip или residual конекције
  - Оригинално, shortcut конекције су предложене у дубоким мрежама за компјутерски вид (residual мреже) да ублаже проблеме нестајања градијента
  - Shortcut конекција креира алтернативни краћи пут за градијент да тече кроз мрежу
  - Ово се постиже додавањем излаза једног слоја излазу каснијег слоја, обично прескакањем једног или више слојева између
  - Хајде да илуструјемо ову идеју са малим примером мреже:

  Cell 14cfd241-a32e-4601-8790-784b82f2f23e:
  - У коду, изгледа овако:

  Cell b39bf277-b3db-4bb1-84ce-7a20caff1011:
  - Хајде да испишемо вредности градијента прво без shortcut конекција:

  Cell 837fd5d4-7345-4663-97f5-38f19dfde621:
  - Даље, хајде да испишемо вредности градијента са shortcut конекцијама:

  Cell 79ff783a-46f0-49c5-a7a9-26a525764b6e:
  - Као што можемо видети на основу излаза горе, shortcut конекције спречавају градијенте да нестану у раним слојевима (према layer.0)
  - Користићемо овај концепт shortcut конекције следеће када имплементирамо трансформер блок

  Cell cae578ca-e564-42cf-8635-a2267047cdff:
  4.5 Повезивање пажње и линеарних слојева у трансформер блоку

  Cell a3daac6f-6545-4258-8f2d-f45a7394f429:
  - У овом одељку, сада комбинујемо претходне концепте у такозвани трансформер блок
  - Трансформер блок комбинује каузални више-глави пажњу модул из претходног поглавља са линеарним слојевима, feed forward неуронском мрежом коју смо имплементирали у ранијем одељку
  - Поред тога, трансформер блок такође користи dropout и shortcut конекције

  Cell 54d2d375-87bd-4153-9040-63a1e6a2b7cb:
  - Претпоставимо да имамо 2 улазна узорка са 6 токена сваки, где је сваки токен 768-димензионални вектор уграђивања; онда овај трансформер блок примењује самопажњу, праћену линеарним слојевима, да произведе излаз
  сличне величине
  - Можете мислити на излаз као на аугментирану верзију контекст вектора о којима смо разговарали у претходном поглављу

  Cell 46618527-15ac-4c32-ad85-6cfea83e006e:
  4.6 Кодирање GPT модела

  Cell dec7d03d-9ff3-4ca3-ad67-01b67c2f5457:
  - Скоро смо стигли: сада укључимо трансформер блок у архитектуру коју смо кодирали на самом почетку овог поглавља тако да добијемо употребљиву GPT архитектуру
  - Имајте на уму да се трансформер блок понавља више пута; у случају најмањег 124M GPT-2 модела, понављамо га 12 пута:

  Cell 324e4b5d-ed89-4fdf-9a52-67deee0593bc:
  - Одговарајућа имплементација кода, где је cfg["n_layers"] = 12:

  Cell 2750270f-c45d-4410-8767-a6adbd05d5c3:
  - Користећи конфигурацију модела са 124M параметара, сада можемо инстанцирати овај GPT модел са случајним почетним тежинама на следећи начин:

  Cell 6d616e7a-568b-4921-af29-bd3f4683cd2e:
  - Тренираћемо овај модел у следећем поглављу
  - Међутим, брза напомена о његовој величини: претходно смо га називали моделом са 124M параметара; можемо двоструко проверити овај број на следећи начин:

  Cell b67d13dd-dd01-4ba6-a2ad-31ca8a9fd660:
  - Као што видимо горе, овај модел има 163M, а не 124M параметара; зашто?
  - У оригиналном GPT-2 раду, истраживачи су применили weight tying, што значи да су поново користили слој уграђивања токена (tok_emb) као излазни слој, што значи постављање self.out_head.weight = self.tok_emb.weight      
  - Слој уграђивања токена пројектује 50,257-димензионалне one-hot кодиране улазне токене на 768-димензионалну репрезентацију уграђивања
  - Излазни слој пројектује 768-димензионална уграђивања назад у 50,257-димензионалну репрезентацију тако да можемо претворити ове назад у речи (више о томе у следећем одељку)
  - Дакле, слој уграђивања и излазни слој имају исти број параметара тежина, као што можемо видети на основу облика њихових матрица тежина
  - Међутим, брза напомена о његовој величини: претходно смо га називали моделом са 124M параметара; можемо двоструко проверити овај број на следећи начин:

  Cell f02259f6-6f79-4c89-a866-4ebeae1c3289:
  - У оригиналном GPT-2 раду, истраживачи су поново користили матрицу уграђивања токена као излазну матрицу
  - Одговарајуће, ако бисмо одузели број параметара излазног слоја, добили бисмо модел са 124M параметара:

  Cell 40b03f80-b94c-46e7-9d42-d0df399ff3db:
  - У пракси, открио сам да је лакше тренирати модел без weight-tying-а, што је разлог зашто га нисмо имплементирали овде
  - Међутим, вратићемо се и применити ову weight-tying идеју касније када учитавамо предтреиране тежине у поглављу 5
  - Коначно, можемо израчунати меморијске захтеве модела на следећи начин, што може бити корисна референтна тачка:

  Cell 309a3be4-c20a-4657-b4e0-77c97510b47c:
  - Вежба: можете пробати следеће друге конфигурације, које су референциране у https://scholar.google.com/citations?view_op=view_citation&hl=en&user=dOad5HoAAAAJ&citation_for_view=dOad5HoAAAAJ:YsMSGLbcyi4C, такође.        

    - GPT2-small (124M конфигурација коју смо већ имплементирали):
        - "emb_dim" = 768
      - "n_layers" = 12
      - "n_heads" = 12
    - GPT2-medium:
        - "emb_dim" = 1024
      - "n_layers" = 24
      - "n_heads" = 16
    - GPT2-large:
        - "emb_dim" = 1280
      - "n_layers" = 36
      - "n_heads" = 20
    - GPT2-XL:
        - "emb_dim" = 1600
      - "n_layers" = 48
      - "n_heads" = 25

  Cell da5d9bc0-95ab-45d4-9378-417628d86e35:
  4.7 Генерисање текста

  Cell 48da5deb-6ee0-4b9b-8dd2-abed7ed65172:
  - LLM-ови као што је GPT модел који смо имплементирали горе се користе за генерисање једне речи по једном

  Cell a7061524-a3bd-4803-ade6-2e3b7b79ac13:
  - Следећа функција generate_text_simple имплементира greedy декодирање, што је једноставан и брз метод за генерисање текста
  - У greedy декодирању, на сваком кораку, модел бира реч (или токен) са највећом вероватноћом као свој следећи излаз (највиши logit одговара највећој вероватноћи, тако да технички не бисмо чак морали израчунати
  softmax функцију експлицитно)
  - У следећем поглављу, имплементираћемо напреднију generate_text функцију
  - Слика испод приказује како GPT модел, дат улазни контекст, генерише следећи токен речи

  Cell 6515f2c1-3cc7-421c-8d58-cc2f563b7030:
  - generate_text_simple горе имплементира итеративни процес, где креира један токен по једном

  Cell f682eac4-f9bd-438b-9dec-6b1cc7bc05ce:
  - Хајде да припремимо улазни пример:

  Cell 1d131c00-1787-44ba-bec3-7c145497b2c3:
  - Уклони batch димензију и конвертуј назад у текст:

  Cell 9a894003-51f6-4ccc-996f-3b9c7d5a1d70:
  - Имајте на уму да модел није трениран; стога случајни излазни текстови горе
  - Тренираћемо модел у следећем поглављу

  Cell a35278b6-9e5c-480f-83e5-011a1173648f:
  Резиме и закључци

  - Погледајте ./gpt.py скрипт, самосталну скрипту која садржи GPT модел који имплементирамо у овом Jupyter бележнику
  - Решења вежби можете наћи у ./exercise-solutions.ipynb